### Deep Learning

深度学习是机器学习的一个分支，涉及多层神经网络。以下是深度学习的基本概念和原理总结：

- **神经网络的基础**：神经网络由多个神经元组成，神经元通过权重、偏置等参数进行运算。核心组件包括：
  - **神经元**：每个神经元接受多个输入，通过权重加权求和并加上偏置，然后经过激活函数处理，产生输出。
  - **激活函数**：常用的激活函数包括 ReLU（Rectified Linear Unit）、Sigmoid 和 Tanh，用于引入非线性并使模型能够表示复杂的函数。
  - **损失函数**：损失函数衡量模型的预测结果与实际值之间的差距，常用的损失函数有均方误差（MSE）和交叉熵损失（Cross-Entropy Loss）。

- **前馈神经网络（Feedforward Neural Networks）**：这是最基本的深度学习网络结构，信息从输入层传递到隐藏层，再到输出层，不存在反馈环。模型通过调整权重和偏置来最小化损失函数的值。

- **梯度下降与优化算法**：用于训练神经网络的优化技术。通过计算损失函数相对于每个参数的梯度，更新参数以逐步减小损失函数。常用的优化算法包括：
  - **随机梯度下降（SGD）**：通过小批量样本的梯度来更新模型参数，减少计算量。
  - **Adam**：结合了动量和自适应学习率的优化算法，具有较好的收敛性和稳定性。
  - **RMSprop**：基于动量方法的优化算法，适用于处理噪声较大的数据。

- **正则化技术**：为了防止模型在训练过程中过拟合，常用的正则化技术包括：
  - **Dropout**：在训练过程中随机忽略部分神经元，减少模型对特定神经元的依赖。
  - **L2 正则化**：在损失函数中加入权重的 L2 范数，防止权重过大，减小模型复杂度。

### 数学公式

1. **神经元计算公式**：

    \[
    y = \sigma \left( \sum_{i=1}^{n} w_i x_i + b \right)
    \]

    其中：
    - \( x_i \) 是输入，
    - \( w_i \) 是权重，
    - \( b \) 是偏置，
    - \( \sigma(\cdot) \) 是激活函数。

2. **损失函数（均方误差）**：

    \[
    L_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
    \]

    - \( y_i \) 是真实值，
    - \( \hat{y}_i \) 是预测值。

    对于分类问题，交叉熵损失函数为：

    \[
    L_{\text{Cross-Entropy}} = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)
    \]

3. **梯度下降更新规则**：

    \[
    w = w - \eta \cdot \frac{\partial L}{\partial w}
    \]

    - \( w \) 是权重参数，
    - \( \eta \) 是学习率，
    - \( \frac{\partial L}{\partial w} \) 是损失函数对权重的梯度。

4. **随机梯度下降（SGD）**：

    \[
    w = w - \eta \cdot \frac{\partial L_{\text{batch}}}{\partial w}
    \]

    - \( L_{\text{batch}} \) 是基于当前小批量数据的损失函数。

5. **Adam 优化算法**：

    \[
    m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla L_t
    \]
    \[
    v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla L_t)^2
    \]
    \[
    \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    \]
    \[
    w_{t+1} = w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
    \]

    - \( m_t \) 和 \( v_t \) 分别是一阶矩估计（动量）和二阶矩估计，
    - \( \beta_1 \) 和 \( \beta_2 \) 是动量衰减系数，
    - \( \epsilon \) 是一个小常数，用于数值稳定性。

6. **正则化**：
    - **L2 正则化**：

    \[
    L = L_{\text{original}} + \lambda \sum_{i=1}^{n} w_i^2
    \]

    - \( \lambda \) 是正则化系数，
    - \( L_{\text{original}} \) 是原始损失函数。

---

**[返回](../../README.md)**